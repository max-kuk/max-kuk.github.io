---
---

@string{aps = {American Physical Society,}}


@inproceedings{Fiedler2025,
  title = {Optimization of the purity analysis of rapeseed by using hyperspectral imaging in the spectral range from 400 to 1600 nm},
  doi = {10.1117/12.3041814},
  booktitle = {Photonic Technologies in Plant and Agricultural Science II},
  publisher = {SPIE},
  author = {Fiedler,  Johannes and Kukushkin,  Maxim and Kuckelsberg,  Karl Ruben and Storck,  Jan Lukas and Bogdan,  Martin and Schmid,  Thomas and Kaschuba,  Reinhard},
  editor = {Heinemann,  Dag and Polder,  Gerrit},
  year = {2025},
  month = mar,
  pages = {12},
  bibtex_show = {true},
}


@inproceedings{Brookes2025,
  author = {Brookes*, Otto and Kukushkin*, Maksim and Mirmehdi, Majid and Stephens, Colleen and Dieguez, Paula and Hicks, Thurston C. and Jones, Sorrel and Lee, Kevin and McCarthy, Maureen S. and Meier, Amelia and Normand, Emmanuelle and Wessling, Erin G. and Wittig, Roman M. and Langergraber, Kevin and Zuberbühler, Klaus and Boesch, Lukas and Schmid, Thomas and Arandjelovic, Mimi and Kühl, Hjalmar and Burghardt, Tilo},
  title  = {The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025},
  html = {https://arxiv.org/abs/2502.21201},
  bibtex_show = {true},
  selected={true},
  abstract={Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42% mAP for convolutional and +3.75% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos).},
  website = {https://obrookes.github.io/panaf-fgbg.github.io/},
  annotation={* Equal technical contribution},
}

@inproceedings{Kukushkin2021,
  author = {Kukushkin, Maksim and Ntalampiras, Stavros},
  title = {Automatic acoustic classification of feline sex},
  year = {2021},
  isbn = {9781450385695},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3478384.3478385},
  abstract = {This paper presents a novel method for classifying the feline sex based on the respective vocalizations. Due to the size of the available dataset, we rely on tree-based classifiers which can efficiently learn classification rules in such poor data availability cases. More specifically, this work investigates the ability of random forests and boosting classifiers when trained with a wide range of acoustic features derived both from time and frequency domain. The considered classifiers are evaluated using standardized figures of merit including f1-score, recall, precision, and accuracy. The best-performing classifier was the CatBoost, while the obtained results are in line with the state-of-the-art accuracy levels in the field of animal sex classification.},
  booktitle = {Proceedings of the 16th International Audio Mostly Conference},
  pages = {156–160},
  numpages = {5},
  keywords = {tree-based classifiers, domestic animals, bioacoustics, audio signal processing, audio pattern recognition, Internet of Audio Things.},
  location = {virtual/Trento, Italy},
  series = {AM '21},
  html = {https://dl.acm.org/doi/10.1145/3478384.3478385},
  bibtex_show = {true},
}

@article{Kukushkin2021,
  author = {Kukushkin,  Maksim and Enders,  Matthias and Kaschuba,  Reinhard and Bogdan,  Martin and Schmid,  Thomas},
  keywords = {anomaly detection,  seed production,  hyperspectral imaging,  autoencoder},
  title = {Canola seed or not? Autoencoder-based Anomaly Detection in Agricultural Seed Production},
  publisher = {Gesellschaft f\"{u}r Informatik e.V.},
  year = {2023},
  doi = {10.18420/INF2023_169},
  bibtex_show = {true},
  html = {https://dl.gi.de/items/020df413-5c97-4677-a43c-1c2bad4fb8cf},
  abstract = {Analysing harvested seeds is a time-consuming task in the seed-producing industry. Automating this process has the potential to enhance and expedite agricultural seed production. In our study, we focus on differentiating Canola seeds from visually similar non-Canola seeds using computer vision techniques. Our approach utilises both RGB and hyperspectral images, captured by a specialised camera, to train separate autoencoder neural networks. By leveraging the high spatial resolution of RGB data and the high spectral resolution of hyperspectral data, we develop distinct models for Canola seed analysis, ensuring a comprehensive and robust assessment. The autoencoder networks are trained on a dataset of Canola seeds, allowing for the extraction of latent representations from both RGB and hyperspectral data. This enables efficient compression of input data and effective discrimination between Canola and non-Canola seeds. Our proposed approach demonstrates promising results in detecting non-Canola seeds in unseen test data.}
}

@inproceedings{kukushkin2024bimae,
  title={BiMAE-A Bimodal Masked Autoencoder Architecture for Single-Label Hyperspectral Image Classification},
  author={Kukushkin, Maksim and Bogdan, Martin and Schmid, Thomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  pages={2987--2996},
  year={2024},
  bibtex_show = {true},
  selected={true},
  html = {https://openaccess.thecvf.com/content/CVPR2024W/PBVS/html/Kukushkin_BiMAE_-_A_Bimodal_Masked_Autoencoder_Architecture_for_Single-Label_Hyperspectral_CVPRW_2024_paper.html},
  abstract = {Hyperspectral imaging offers manifold opportunities for applications that may not, or only partially, be achieved within the visual spectrum. Our paper presents a novel approach for Single-Label Hyperspectral Image Classification, demonstrated through the example of a key challenge faced by agricultural seed producers: seed purity testing. We employ Self-Supervised Learning and Masked Image Modeling techniques to tackle this task. Recognizing the challenges and costs associated with acquiring hyperspectral data, we aim to develop a versatile method capable of working with visible, arbitrary combinations of spectral bands (multispectral data) and hyperspectral sensor data. By integrating RGB and hyperspectral data, we leverage the detailed spatial information from RGB images and the rich spectral information from hyperspectral data to enhance the accuracy of seed classification. Through evaluations in various real-life scenarios, we demonstrate the flexibility, scalability, and efficiency of our approach.},
  poster = {CVPR_2024_Poster.pdf},
  code = {https://github.com/max-kuk/bimae_seed_classification}
}

@inproceedings{Kukushkin2023morphological,
  title = {On optimizing morphological neural networks for hyperspectral image classification},
  booktitle = {Sixteenth International Conference on Machine Vision (ICMV 2023)},
  publisher = {SPIE},
  author = {Kukushkin,  Maksim and Bogdan,  Martin and Schmid,  Thomas},
  editor = {Osten,  Wolfgang},
  year = {2024},
  month = apr,
  bibtex_show = {true},
  doi = {10.1117/12.3023593},
  html = {https://neurophotonics.spiedigitallibrary.org/conference-proceedings-of-spie/13072/1307202/On-optimizing-morphological-neural-networks-for-hyperspectral-image-classification/10.1117/12.3023593.short},
  abstract = {Convolutional Neural Networks have become an important tool for various Computer Vision tasks. Yet, increasing complexity of such architectures drives computational costs. To this end, we propose two measures to achieve similar classification results as state-of-the-art architectures while at the same time reducing model complexity significantly. Firstly, we describe a novel type of non-linear parameter-efficient morphological layers inspired by concepts that are well-known and widely used with convolutions. Secondly, we present a set of simple network architectures, organized as optimization framework, which is enhanced by neural architecture search and hyperparameter optimization. In experiments with hyperspectral remote sensing data, we demonstrate that the identified optimal morphological architecture produces results not only comparable with other architectures from the optimization framework, but also comparable or better than selected state-of-the-art neural network architectures for image classification. Depending on the performed task, the proposed optimized architecture requires up to 25 times fewer parameters than actual state-of-the-art networks.}
}

@inproceedings{Erichsmeier2024,
  title = {Automating the purity analysis of oilseed rape through usage of hyperspectral imaging},
  booktitle = {Photonic Technologies in Plant and Agricultural Science},
  publisher = {SPIE},
  author = {Erichsmeier,  Fabian and Kukushkin,  Maksim and Fiedler,  Johannes and Enders,  Matthias and Goertz,  Simon and Bogdan,  Martin and Schmid,  Thomas and Kaschuba,  Reinhard},
  editor = {Heinemann,  Dag and Polder,  Gerrit},
  year = {2024},
  month = mar,
  bibtex_show = {true},
  doi = {10.1117/12.3002665},
  html = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12879/3002665/Automating-the-purity-analysis-of-oilseed-rape-through-usage-of/10.1117/12.3002665.short},
  abstract = {The purity analysis of oilseed rape (Brassica napus L.) is currently a labor-intensive and manual process, requiring significant human effort for accurate assessment. In this context, the KIRa-Sorter system presents an innovative solution that leverages hyperspectral imaging technology for automating the comprehensive classification of various contaminants present in rapeseed samples. The initial phase of the KIRa-Sorter system involves the efficient capture of hyperspectral and RGB image data from rapeseed samples as input for classification. From up to 200 different types of foreign objects typically found in these samples, a reduced coreset has been defined that the system is able to automatically singulate, classify and physically sort.}
}

@inproceedings{Kukushkin2024bicae,
  title = {BiCAE - A Bimodal Convolutional Autoencoder for Seed Purity Testing},
  booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track (ECML-PKDD 2024)},
  author = {Kukushkin, Maksim and Bogdan, Martin and Schmid, Thomas},
  editor = {Bifet, Albert and Krilavivcius, Tomas and Miliou, Ioanna and Nowaczyk, Slawomir},
  year = {2024},
  month = aug,
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  pages = {447--462},
  bibtex_show = {true},
  doi = {10.1007/978-3-031-70381-2_28},
  isbn = {978-3-031-70381-2},
  html = {https://link.springer.com/chapter/10.1007/978-3-031-70381-2_28},
  selected = {true},
  poster = {https://drive.proton.me/urls/1CS8BAZ2ZM#YHmhcPfxXCqp},
  abstract = {In the seed-producing industry, accurate assessment of harvested seeds for technical purity is a necessary, yet time-consuming and labor-intensive task. Automating this task holds immense potential for enhancing agricultural seed productivity, and using computer vision methods to classify seeds has already demonstrated promising results. Here, we propose a novel spectral-enhanced image anomaly detection approach to accurately discriminate Canola seeds (Brassica napus L.) from visually similar non-Canola seeds. Our bimodal approach exploits both RGB and data captured by a hyperspectral camera of the same sample. For efficient processing of this data, we suggest a novel bimodal convolutional autoencoder (BiCAE) architecture, which combines the strengths of high spatial resolution in RGB and high spectral resolution in hyperspectral data. We demonstrate that training our BiCAE model on a Canola dataset allows to learn a joint latent representation that effectively extracts spatio-spectral information from both RGB and hyperspectral data. Experiments show promising results in differentiating between Canola and non-Canola samples, in particular in detecting various types of non-Canola seeds in previously unseen test data. The obtained results highlight the model's ability to generalize beyond the training data, surpassing unimodal models that rely solely on a single modality.},
}

@article{Kholiavchenko2024,
  title = {Deep dive into KABR: a dataset for understanding ungulate behavior from in-situ drone video},
  html = {https://link.springer.com/article/10.1007/s11042-024-20512-4},
  doi = {10.1007/s11042-024-20512-4},
  journal = {Multimedia Tools and Applications},
  publisher = {Springer Science and Business Media LLC},
  author = {Kholiavchenko,  Maksim and Kline,  Jenna and Kukushkin,  Maksim and Brookes,  Otto and Stevens,  Sam and Duporge,  Isla and Sheets,  Alec and Babu,  Reshma R. and Banerji,  Namrata and Campolongo,  Elizabeth and Thompson,  Matthew and Tiel,  Nina Van and Miliko,  Jackson and Bessa,  Eduardo and Mirmehdi,  Majid and Schmid,  Thomas and Berger-Wolf,  Tanya and Rubenstein,  Daniel I. and Burghardt,  Tilo and Stewart,  Charles V.},
  year = {2024},
  month = dec,
  selected = {true},
  bibtex_show = {true},
  website = {https://kabrdata.xyz},
  abstract = {In this paper, we extend the dataset statistics, model benchmarks, and performance analysis for the recently published KABR dataset, an in situ dataset for ungulate behavior recognition using aerial footage from the Mpala Research Centre in Kenya. The dataset comprises video footage of reticulated giraffes (lat. Giraffa reticulata), Plains zebras (lat. Equus quagga), and Grévy’s zebras (lat. Equus grevyi) captured using a DJI Mavic 2S drone. It includes both spatiotemporal (i.e., mini-scenes) and behavior annotations provided by an expert behavioral ecologist. In total, KABR has more than 10 hours of annotated video. We extend the previous work in four key areas by: (i) providing comprehensive dataset statistics to reveal new insights into the data distribution across behavior classes and species; (ii) extending the set of existing benchmark models to include a new state-of-the-art transformer; (iii) investigating weight initialization strategies and exploring whether pretraining on human action recognition datasets is transferable to in situ animal behavior recognition directly (i.e., zero-shot) or as initialization for end-to-end model training; and (iv) performing a detailed statistical analysis of the performance of these models across species, behavior, and formally defined segments of the long-tailed distribution. The KABR dataset addresses the limitations of previous datasets sourced from controlled environments, offering a more authentic representation of natural animal behaviors. This work marks a significant advancement in the automatic analysis of wildlife behavior, leveraging drone technology to overcome traditional observational challenges and enabling a more nuanced understanding of animal interactions in their natural habitats. The dataset is available at https://kabrdata.xyz}
}