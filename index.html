<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="UKQsQE2CjYVPcd5nSvPYT7oVLkquNtsNpbzfPWtOE3s.html"> <meta name="msvalidate.01" content="BingSiteAuth.xml"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maksim Kukushkin </title> <meta name="author" content="Maksim Kukushkin"> <meta name="description" content="PhD Candidate at Leipzig University "> <meta name="keywords" content="computer vision, machine learning, deep learning, research, leipzig university, leipzig, halle, halle university, germany"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?46745b64199a42ffe16f7fdf25862f59"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://max-kuk.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="https://orcid.org/0009-0006-1014-6339" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=C3_VIssAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Maksim_Kukushkin/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/max-kuk" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/maksim-kukushkin" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.flickr.com/max-kuk" title="Flickr" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-flickr"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Datasets </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://max-kuk.github.io/bisid-5k/">BiSID-5k</a> <a class="dropdown-item " href="https://obrookes.github.io/panaf-fgbg.github.io/" rel="external nofollow noopener" target="_blank">PanAf-FGBG</a> <a class="dropdown-item " href="https://kabrdata.xyz/" rel="external nofollow noopener" target="_blank">KABR</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Maksim</span> Kukushkin </h1> <p class="desc">PhD Candidate @<a href="https://www.uni-leipzig.de" rel="external nofollow noopener" target="_blank">Leipzig University</a> and Research Associate @<a href="https://www.uni-halle.de" rel="external nofollow noopener" target="_blank">Martin Luther University Halle-Wittenberg</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?62b853308ee95ea0a09858c5db052eba" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Magdeburger Str. 8</p> <p>DE-06112, Halle (Saale), Germany</p> </div> </div> <div class="clearfix"> <p>Salut! Iâ€™m Maksim Kukushkin. I am pursuing my PhD in the <a href="https://nmi.informatik.uni-leipzig.de/" rel="external nofollow noopener" target="_blank"> Neuromorphic Information Processing Department</a> at Leipzig University. Additionally, I work as a Research Associate in the Digital Research Methods in Medicine Group at Martin Luther University Halle-Wittenberg. My current research focuses on analysing multidimensional dataâ€”particularly hyperspectral, video, and medical dataâ€”for which I develop classification algorithms.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 28, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2502.21201" rel="external nofollow noopener" target="_blank">The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</a> has been accepted to <a href="https://cvpr.thecvf.com" rel="external nofollow noopener" target="_blank">CVPR 2025 (oral)</a> in Nashville, USA ðŸ‡ºðŸ‡¸ ðŸŽ‰ðŸŽ‰ðŸŽ‰! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 21, 2024</th> <td> Our paper <a href="https://link.springer.com/article/10.1007/s11042-024-20512-4" rel="external nofollow noopener" target="_blank">Deep dive into KABR: a dataset for understanding ungulate behavior from in-situ drone video</a> has been accepted to <a href="https://link.springer.com/journal/11042" rel="external nofollow noopener" target="_blank">Multimedia Tools and Applications (Springer)</a> ðŸŽ‰ðŸŽ‰ðŸŽ‰. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> Started work at <a href="https://www.uni-halle.de/" rel="external nofollow noopener" target="_blank">Martin Luther University Halle-Wittenberg</a> as a Research Associate in the <a href="https://www.umh.de/forschung/nachwuchsgruppen/nwg-schmid" rel="external nofollow noopener" target="_blank"> Digital Methods in Medicine </a> group. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 01, 2024</th> <td> I am pleased to share that I have just joined <a href="https://uob-mavi.github.io/people/" rel="external nofollow noopener" target="_blank">MaVi Research Group</a> at the <a href="https://www.bristol.ac.uk" rel="external nofollow noopener" target="_blank">University of Bristol</a> for the next three months. During this time, I will be working on a project focused on behavioural animal analysis. Looking forward to the experience and the opportunity to contribute to the research. </td> </tr> <tr> <th scope="row" style="width: 20%">May 27, 2024</th> <td> Our paper <a href="https://link.springer.com/chapter/10.1007/978-3-031-70381-2_28" rel="external nofollow noopener" target="_blank">BiCAE - A Bimodal Convolutional Autoencoder for Seed Purity Testing</a> has been accepted to <a href="https://ecmlpkdd.org/2024/" rel="external nofollow noopener" target="_blank">ECML-PKDD 2024</a> in Vilnius, Lithuania ðŸ‡±ðŸ‡¹ ðŸŽ‰ðŸŽ‰ðŸŽ‰. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Conference</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/background_compensation-480.webp 480w,/assets/img/publication_preview/background_compensation-800.webp 800w,/assets/img/publication_preview/background_compensation-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/background_compensation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="background_compensation.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Brookes2025" class="col-sm-8"> <div class="title">The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</div> <div class="author"> <a href="https://obrookes.github.io/otto.github.io/" rel="external nofollow noopener" target="_blank">Otto Brookes<sup>*</sup></a>,Â <em>Maksim Kukushkin<sup>*</sup></em>,Â <a href="https://majidmirmehdi.github.io/" rel="external nofollow noopener" target="_blank">Majid Mirmehdi</a>,Â Colleen Stephens,Â Paula Dieguez,Â Thurston C. Hicks,Â Sorrel Jones, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus ZuberbÃ¼hler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar KÃ¼hl, Tilo Burghardt' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Equal technical contribution"> </i> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper Award Candidate</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.21201" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://obrookes.github.io/panaf-fgbg.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award Candidate</p> </div> <div class="abstract hidden"> <p>Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42% mAP for convolutional and +3.75% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Brookes2025</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brookes, Otto and Kukushkin, Maksim and Mirmehdi, Majid and Stephens, Colleen and Dieguez, Paula and Hicks, Thurston C. and Jones, Sorrel and Lee, Kevin and McCarthy, Maureen S. and Meier, Amelia and Normand, Emmanuelle and Wessling, Erin G. and Wittig, Roman M. and Langergraber, Kevin and ZuberbÃ¼hler, Klaus and Boesch, Lukas and Schmid, Thomas and Arandjelovic, Mimi and KÃ¼hl, Hjalmar and Burghardt, Tilo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bimae-480.webp 480w,/assets/img/publication_preview/bimae-800.webp 800w,/assets/img/publication_preview/bimae-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/bimae.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bimae.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="kukushkin2024bimae" class="col-sm-8"> <div class="title">BiMAE-A Bimodal Masked Autoencoder Architecture for Single-Label Hyperspectral Image Classification</div> <div class="author"> <em>Maksim Kukushkin</em>,Â <a href="https://nmi.informatik.uni-leipzig.de/staff/prof-dr-martin-bogdan/" rel="external nofollow noopener" target="_blank">Martin Bogdan</a>,Â andÂ <a href="https://nmi.informatik.uni-leipzig.de/ml-group/staff/thomas-schmid/" rel="external nofollow noopener" target="_blank">Thomas Schmid</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2024W/PBVS/html/Kukushkin_BiMAE_-_A_Bimodal_Masked_Autoencoder_Architecture_for_Single-Label_Hyperspectral_CVPRW_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/max-kuk/bimae_seed_classification" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://drive.proton.me/urls/QMEQRRV4PM#ees8eDoJHbOk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Hyperspectral imaging offers manifold opportunities for applications that may not, or only partially, be achieved within the visual spectrum. Our paper presents a novel approach for Single-Label Hyperspectral Image Classification, demonstrated through the example of a key challenge faced by agricultural seed producers: seed purity testing. We employ Self-Supervised Learning and Masked Image Modeling techniques to tackle this task. Recognizing the challenges and costs associated with acquiring hyperspectral data, we aim to develop a versatile method capable of working with visible, arbitrary combinations of spectral bands (multispectral data) and hyperspectral sensor data. By integrating RGB and hyperspectral data, we leverage the detailed spatial information from RGB images and the rich spectral information from hyperspectral data to enhance the accuracy of seed classification. Through evaluations in various real-life scenarios, we demonstrate the flexibility, scalability, and efficiency of our approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kukushkin2024bimae</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BiMAE-A Bimodal Masked Autoencoder Architecture for Single-Label Hyperspectral Image Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kukushkin, Maksim and Bogdan, Martin and Schmid, Thomas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2987--2996}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Conference</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bicae-480.webp 480w,/assets/img/publication_preview/bicae-800.webp 800w,/assets/img/publication_preview/bicae-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/bicae.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bicae.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kukushkin2024bicae" class="col-sm-8"> <div class="title">BiCAE - A Bimodal Convolutional Autoencoder for Seed Purity Testing</div> <div class="author"> <em>Maksim Kukushkin</em>,Â <a href="https://nmi.informatik.uni-leipzig.de/staff/prof-dr-martin-bogdan/" rel="external nofollow noopener" target="_blank">Martin Bogdan</a>,Â andÂ <a href="https://nmi.informatik.uni-leipzig.de/ml-group/staff/thomas-schmid/" rel="external nofollow noopener" target="_blank">Thomas Schmid</a> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-70381-2_28" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.proton.me/urls/1CS8BAZ2ZM#YHmhcPfxXCqp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>In the seed-producing industry, accurate assessment of harvested seeds for technical purity is a necessary, yet time-consuming and labor-intensive task. Automating this task holds immense potential for enhancing agricultural seed productivity, and using computer vision methods to classify seeds has already demonstrated promising results. Here, we propose a novel spectral-enhanced image anomaly detection approach to accurately discriminate Canola seeds (Brassica napus L.) from visually similar non-Canola seeds. Our bimodal approach exploits both RGB and data captured by a hyperspectral camera of the same sample. For efficient processing of this data, we suggest a novel bimodal convolutional autoencoder (BiCAE) architecture, which combines the strengths of high spatial resolution in RGB and high spectral resolution in hyperspectral data. We demonstrate that training our BiCAE model on a Canola dataset allows to learn a joint latent representation that effectively extracts spatio-spectral information from both RGB and hyperspectral data. Experiments show promising results in differentiating between Canola and non-Canola samples, in particular in detecting various types of non-Canola seeds in previously unseen test data. The obtained results highlight the modelâ€™s ability to generalize beyond the training data, surpassing unimodal models that rely solely on a single modality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kukushkin2024bicae</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BiCAE - A Bimodal Convolutional Autoencoder for Seed Purity Testing}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kukushkin, Maksim and Bogdan, Martin and Schmid, Thomas}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bifet, Albert and Krilavivcius, Tomas and Miliou, Ioanna and Nowaczyk, Slawomir}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{447--462}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-031-70381-2_28}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-70381-2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kabr-480.webp 480w,/assets/img/publication_preview/kabr-800.webp 800w,/assets/img/publication_preview/kabr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/kabr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kabr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Kholiavchenko2024" class="col-sm-8"> <div class="title">Deep dive into KABR: a dataset for understanding ungulate behavior from in-situ drone video</div> <div class="author"> <a href="https://orcid.org/0000-0001-6757-1957" rel="external nofollow noopener" target="_blank">Maksim Kholiavchenko</a>,Â <a href="https://scholar.google.com/citations?user=bhNDuigAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jenna Kline</a>,Â <em>Maksim Kukushkin</em>,Â <a href="https://obrookes.github.io/otto.github.io/" rel="external nofollow noopener" target="_blank">Otto Brookes</a>,Â Sam Stevens,Â Isla Duporge,Â Alec Sheets, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Reshma R. Babu, Namrata Banerji, Elizabeth Campolongo, Matthew Thompson, Nina Van Tiel, Jackson Miliko, Eduardo Bessa, Majid Mirmehdi, Thomas Schmid, Tanya Berger-Wolf, Daniel I. Rubenstein, Tilo Burghardt, Charles V. Stewart' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> </div> <div class="periodical"> <em>Multimedia Tools and Applications</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11042-024-20512-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://kabrdata.xyz" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this paper, we extend the dataset statistics, model benchmarks, and performance analysis for the recently published KABR dataset, an in situ dataset for ungulate behavior recognition using aerial footage from the Mpala Research Centre in Kenya. The dataset comprises video footage of reticulated giraffes (lat. Giraffa reticulata), Plains zebras (lat. Equus quagga), and GrÃ©vyâ€™s zebras (lat. Equus grevyi) captured using a DJI Mavic 2S drone. It includes both spatiotemporal (i.e., mini-scenes) and behavior annotations provided by an expert behavioral ecologist. In total, KABR has more than 10 hours of annotated video. We extend the previous work in four key areas by: (i) providing comprehensive dataset statistics to reveal new insights into the data distribution across behavior classes and species; (ii) extending the set of existing benchmark models to include a new state-of-the-art transformer; (iii) investigating weight initialization strategies and exploring whether pretraining on human action recognition datasets is transferable to in situ animal behavior recognition directly (i.e., zero-shot) or as initialization for end-to-end model training; and (iv) performing a detailed statistical analysis of the performance of these models across species, behavior, and formally defined segments of the long-tailed distribution. The KABR dataset addresses the limitations of previous datasets sourced from controlled environments, offering a more authentic representation of natural animal behaviors. This work marks a significant advancement in the automatic analysis of wildlife behavior, leveraging drone technology to overcome traditional observational challenges and enabling a more nuanced understanding of animal interactions in their natural habitats. The dataset is available at https://kabrdata.xyz</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kholiavchenko2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep dive into KABR: a dataset for understanding ungulate behavior from in-situ drone video}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s11042-024-20512-4}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Multimedia Tools and Applications}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Science and Business Media LLC}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kholiavchenko, Maksim and Kline, Jenna and Kukushkin, Maksim and Brookes, Otto and Stevens, Sam and Duporge, Isla and Sheets, Alec and Babu, Reshma R. and Banerji, Namrata and Campolongo, Elizabeth and Thompson, Matthew and Tiel, Nina Van and Miliko, Jackson and Bessa, Eduardo and Mirmehdi, Majid and Schmid, Thomas and Berger-Wolf, Tanya and Rubenstein, Daniel I. and Burghardt, Tilo and Stewart, Charles V.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Maksim Kukushkin. Last updated: May 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-EH11GZ27SW"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EH11GZ27SW");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>